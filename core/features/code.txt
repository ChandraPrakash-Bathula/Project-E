Folder Structure
--------------------------------------------------
./
    utils.py
    code.txt
    __init__.py
    translation.py
    summarization/
        with_slm.py
        with_summarization_pipe.py
        __init__.py
        test.txt
        model/
    zsclassifier/
        classifier.py
        __init__.py
        model/
    tts/
        main.py
        __init__.py
        model/
    keyword_extraction/
        utils.py
        main.py
        __init__.py
        model/
    asr/
        model.py
        video_transcription.py
        audio_transcription.py


File Contents
--------------------------------------------------


./utils.py
File type: .py
import pymupdf
import docx
import requests
from bs4 import BeautifulSoup
import torch
from transformers import is_torch_npu_available

def extract_content_from_file(file_paths: list[str] | str):
        text = ""
        
        file_paths = [file_paths] if isinstance(file_paths, str) else file_paths
        try:
            for file_path in file_paths:
                # Handle PDF file
                if file_path.endswith(".pdf"):
                    pdf = pymupdf.open(file_path)
                    for page in pdf:
                        text += page.get_text()

                # Handle word file
                elif file_path.endswith(".docx"):
                    doc = docx.Document(file_path)
                    for para in doc.paragraphs:
                        text += para.text # + "\n"

                # Handle webpage
                elif file_path.startswith("http://") or file_path.startswith("https://"):
                    response = requests.get(file_path)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    text += soup.get_text(separator='\n')

                # Handle plain text files
                elif file_path.endswith(".txt"):
                    with open(file_path, 'r', encoding='utf-8') as file:
                        text += file.read()
                
                # Handle any other file like code file
                try:
                    with open(file_path, 'r') as file:
                        text += file.read()
                except:
                    raise ValueError("Couldn't read the given file: {}".format(file_path))
        
        except Exception as e:
            raise ValueError("Error whil reding the file: {}".format(file_path))

        return text.strip()

def get_current_device():
    """
    Returns the name of the available device
    """
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    elif is_torch_npu_available():
        return "npu"
    else:
        return "cpu"

--------------------------------------------------
File End
--------------------------------------------------


./code.txt
File type: .txt
Folder Structure
--------------------------------------------------
./
    utils.py
    __init__.py
    translation.py
    summarization/
        with_slm.py
        with_summarization_pipe.py
        __init__.py
        test.txt
        model/
    zsclassifier/
        classifier.py
        __init__.py
        model/
    __pycache__/
        utils.cpython-311.pyc
        __init__.cpython-311.pyc
        device.cpython-311.pyc
    tts/
        main.py
        __init__.py
        model/
    keyword_extraction/
        utils.py
        main.py
        __init__.py
        model/
    asr/
        model.py
        video_transcription.py
        audio_transcription.py


File Contents
--------------------------------------------------


./utils.py
File type: .py
import pymupdf
import docx
import requests
from bs4 import BeautifulSoup
import torch
from transformers import is_torch_npu_available

def extract_content_from_file(file_paths: list[str] | str):
        text = ""
        
        file_paths = [file_paths] if isinstance(file_paths, str) else file_paths
        try:
            for file_path in file_paths:
                # Handle PDF file
                if file_path.endswith(".pdf"):
                    pdf = pymupdf.open(file_path)
                    for page in pdf:
                        text += page.get_text()

                # Handle word file
                elif file_path.endswith(".docx"):
                    doc = docx.Document(file_path)
                    for para in doc.paragraphs:
                        text += para.text # + "\n"

                # Handle webpage
                elif file_path.startswith("http://") or file_path.startswith("https://"):
                    response = requests.get(file_path)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    text += soup.get_text(separator='\n')

                # Handle plain text files
                elif file_path.endswith(".txt"):
                    with open(file_path, 'r', encoding='utf-8') as file:
                        text += file.read()
                
                # Handle any other file like code file
                try:
                    with open(file_path, 'r') as file:
                        text += file.read()
                except:
                    raise ValueError("Couldn't read the given file: {}".format(file_path))
        
        except Exception as e:
            raise ValueError("Error whil reding the file: {}".format(file_path))

        return text.strip()

def get_current_device():
    """
    Returns the name of the available device
    """
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    elif is_torch_npu_available():
        return "npu"
    else:
        return "cpu"

--------------------------------------------------
File End
--------------------------------------------------


./__init__.py
File type: .py


--------------------------------------------------
File End
--------------------------------------------------


./translation.py
File type: .py
from googletrans import Translator
translator = Translator(service_urls=["translate.googleapis.com", "translate.googleapis.co.in"], user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64)')

def translate_text(text, target_language, source_language="auto"):
    translated_text = translator.translate(text, dest=target_language, src=source_language)
    return translated_text.text.strip()

--------------------------------------------------
File End
--------------------------------------------------


./summarization/with_slm.py
File type: .py
from transformers import pipeline
import torch
from features.utils import extract_content_from_file
from pydantic import BaseModel, Field
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, AutoConfig, GenerationConfig
from lmformatenforcer import JsonSchemaParser
from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn
import json

class SummarizationOutput(BaseModel):
    summary: str = Field(description="Summary of the content")
character_level_parser = JsonSchemaParser(SummarizationOutput.model_json_schema())

# Initialize the configs
repo_id = "microsoft/Phi-3.5-mini-instruct"
cache_dir="./model/"
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_4bit_compute_dtype=getattr(torch, "float16"),
    bnb_4bit_quant_type="fp4",
    bnb_4bit_use_double_quant=False
)
config = AutoConfig.from_pretrained(repo_id, cache_dir=cache_dir)

# Init the tokenizer
tokenizer = AutoTokenizer.from_pretrained(repo_id)

# Init generation config and model
prefix_allowed_tokens_fn = build_transformers_prefix_allowed_tokens_fn(tokenizer, character_level_parser)

model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path=repo_id,
    torch_dtype="auto",
    # trust_remote_code=True,
    quantization_config=bnb_config,
    config=config,
    device_map="auto",
)
model.eval()

lm_pipline = pipeline("text-generation", model=model, tokenizer=tokenizer, config=config, framework="pt", device_map="auto", prefix_allowed_tokens_fn=prefix_allowed_tokens_fn)

generate_kwargs = dict(
        max_new_tokens=1024,
        do_sample=True,
        output_scores=True,
        return_dict_in_generate=False,
        return_full_text=False,
)

def summarize_with_slm(file_path):
    text = extract_content_from_file(file_path)
    message = [
        {"role": "system", "content": f"You are a Intelligent summarizer, who give a concise summary of the given content by carefully reading throughout them. You MUST answer using the following json schema: {SummarizationOutput.model_json_schema()}"},
        {"role": "user", "content": f"Content: \n\n{text}"}
    ]

    output = lm_pipline(message, **generate_kwargs)
    json_response = output[0]["generated_text"].strip()
    kv = json.loads(json_response)
    return kv["summary"]

--------------------------------------------------
File End
--------------------------------------------------


./summarization/with_summarization_pipe.py
File type: .py
from transformers import pipeline
from features.utils import extract_content_from_file
from features.utils import get_current_device

model_name = "facebook/bart-large-cnn"
summarizer = pipeline('summarization', model=model_name, framework="pt", device=get_current_device(), cache_dir="./model/")

def summarize_summarizartion_pipeline(file_path):
    text = extract_content_from_file(file_path)[:1024]
    summarized = summarizer(text, max_length=int(0.05 * len(str(text))), min_length=int(0.01 * len(str(text))), do_sample=False)
    summary_text = summarized[0]['summary_text']
    return summary_text

--------------------------------------------------
File End
--------------------------------------------------


./summarization/__init__.py
File type: .py
from with_summarization_pipe import summarize_summarizartion_pipeline
from with_slm import summarize_with_slm

--------------------------------------------------
File End
--------------------------------------------------


./summarization/test.txt
File type: .txt
"The Role of Technology in Modern Society"

In today’s rapidly evolving world, technology plays an essential role in almost every aspect of human life. From communication and transportation to healthcare and education, the advancements in technology have fundamentally reshaped how we live, work, and interact with one another. This transformation can be attributed to several key innovations that have occurred over the past few decades, each of which has contributed to the digital landscape we navigate today.

One of the most significant impacts of technology is in the field of communication. The invention of the internet and the proliferation of smartphones have revolutionized how people communicate, both personally and professionally. Gone are the days when communication was limited to phone calls or letters. With the advent of social media platforms, email, and instant messaging apps, people are now connected globally in ways that were once unimaginable. These technological advancements have not only made communication more efficient but have also democratized access to information, allowing individuals in even the most remote corners of the world to engage with content and participate in global conversations.

In addition to communication, technology has had a profound impact on education. With the rise of online learning platforms, students are no longer confined to traditional classrooms. E-learning platforms, such as Coursera, Khan Academy, and edX, offer a wealth of resources that allow individuals to learn at their own pace, from anywhere in the world. This has been especially valuable during the COVID-19 pandemic, as many schools and universities transitioned to online instruction. The use of artificial intelligence (AI) in education has also enhanced the learning experience, with tools that personalize instruction based on a student's strengths and weaknesses.

Healthcare is another domain that has been transformed by technology. Advances in medical research, diagnostics, and treatment have led to significant improvements in patient outcomes. Wearable technology, such as fitness trackers and smartwatches, allows individuals to monitor their health in real-time, promoting proactive healthcare management. Telemedicine has also become increasingly popular, enabling patients to consult with healthcare professionals remotely, thus reducing the need for in-person visits and improving access to care for individuals in underserved areas.

Moreover, technology has reshaped the business landscape. The rise of e-commerce has revolutionized the way we shop, with online platforms like Amazon and Alibaba providing consumers with unprecedented convenience and access to a global marketplace. The gig economy, powered by technology platforms such as Uber, Airbnb, and Fiverr, has created new opportunities for employment and income generation. Additionally, automation and AI have streamlined operations in industries such as manufacturing, logistics, and finance, leading to increased efficiency and productivity.

However, the rapid advancement of technology also presents challenges. Privacy concerns have come to the forefront as more of our personal information is shared online. Cybersecurity threats, including hacking and identity theft, are on the rise, and governments and organizations are struggling to keep up with the evolving tactics of cybercriminals. Additionally, the digital divide remains a significant issue, with many individuals lacking access to the technology and internet services necessary to participate in the digital economy fully.

Despite these challenges, the future of technology holds immense promise. Emerging technologies such as blockchain, quantum computing, and 5G networks have the potential to further transform society in ways we cannot yet fully comprehend. The ongoing development of AI and machine learning will continue to enhance various industries, from healthcare to finance, making processes more efficient and data-driven.

In conclusion, technology has become an integral part of modern society, driving innovation, shaping industries, and changing how we live. While there are challenges to address, the potential benefits of technological advancements are vast, and their continued evolution will undoubtedly play a crucial role in shaping the future of humanity.

--------------------------------------------------
File End
--------------------------------------------------


./zsclassifier/classifier.py
File type: .py
from transformers import pipeline
from features.utils import get_current_device

pipeline = pipeline("zero-shot-classification", model="facebook/bart-large-mnli", device=get_current_device(), cache_dir="./model") # crosslingual: joeddav/xlm-roberta-large-xnli
sentiment_labels = [
            "anxious",
            "worried",
            "confused",
            "neutral",
            "hopeful",
            "relieved",
            "confident",
            "frustrated",
            "overwhelmed",
            "determined",
            "extreme"
        ]

def classify_text(self, text):
    result = self.pipeline(text, self.sentiment_labels)
    classification_result = result['labels'][0][0]
    return classification_result

--------------------------------------------------
File End
--------------------------------------------------


./zsclassifier/__init__.py
File type: .py
from classifier import classify_text

--------------------------------------------------
File End
--------------------------------------------------


./tts/main.py
File type: .py
import torch
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer, AutoFeatureExtractor
import soundfile as sf
import scipy
from features.utils import get_current_device

repo_id = "parler-tts/parler-tts-mini-v1"
attn_implementation = "sdpa" # "sdpa" 1.4x or "flash_attention_2"
device=get_current_device()

model = ParlerTTSForConditionalGeneration.from_pretrained(
            repo_id,
            attn_implementation=attn_implementation,
            cache_dir="./model/"
        ).to(device)
tokenizer = AutoTokenizer.from_pretrained(repo_id, cache_dir="./model/")
feature_extractor = AutoFeatureExtractor.from_pretrained(repo_id, cache_dir="./model/")

description = "Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise."
input_ids = tokenizer(description, return_tensors="pt").to(model.device)

def text2speech(text: str, file_path):
    prompt_input_ids = tokenizer(text, return_tensors="pt").to(model.device)

    generation = model.generate(
        input_ids=input_ids.input_ids,
        attention_mask=input_ids.attention_mask,
        prompt_input_ids=prompt_input_ids.input_ids,
        prompt_attention_mask=prompt_input_ids.attention_mask,
        do_sample=True,
        return_dict_in_generate=True
    )
    audio_data = generation.sequences[0, :generation.audios_length[0]]

    audio_arr = audio_data.cpu().numpy().squeeze()
    scipy.io.wavfile.write(file_path, rate=feature_extractor.sampling_rate, data=audio_arr)

--------------------------------------------------
File End
--------------------------------------------------


./tts/__init__.py
File type: .py
from main import text2speech

--------------------------------------------------
File End
--------------------------------------------------


./keyword_extraction/utils.py
File type: .py
import nltk
from nltk.corpus import words
from nltk.tokenize import word_tokenize, sent_tokenize

nltk.download('punkt')
nltk.download("words")

def split_into_paragraphs(doc: str, tokenizer,  max_tokens_per_para: int = 128):
    sentences = sent_tokenize(doc.strip())
    temp = ""
    temp_list = []
    final_list = []

    for i, sentence in enumerate(sentences):
        sent = sentence
        temp = temp + " " + sent
        wc_temp = len(tokenizer.tokenize(temp))

        if wc_temp < max_tokens_per_para:
            temp_list.append(sentence)

            if i == len(sentences) - 1:
                final_list.append(" ".join(temp_list))

        else:
            final_list.append(" ".join(temp_list))

            temp = sentence
            temp_list = [sentence]

            if i == len(sentences) - 1:
                final_list.append(" ".join(temp_list))

    return [para for para in final_list if len(para.strip()) != 0]

def process_outputs(outputs):
    temp = [output[0].split(" | ") for output in outputs]
    flatten = [item for sublist in temp for item in sublist]
    return sorted(set(flatten), key=flatten.index)

def filter_outputs(key_phrases, text):
    key_phrases = [elem.lower() for elem in key_phrases]
    text = text.lower()

    valid_phrases = []
    invalid_phrases = []

    for phrases in key_phrases:
        for phrase in word_tokenize(phrases):
            if (phrase in word_tokenize(text)) or (phrase in words.words()):
                if phrases not in valid_phrases:
                    valid_phrases.append(phrases)
            else:
                invalid_phrases.append(phrases)

    return [elem for elem in valid_phrases if elem not in invalid_phrases]

--------------------------------------------------
File End
--------------------------------------------------


./keyword_extraction/main.py
File type: .py
from transformers import AutoTokenizer, T5ForConditionalGeneration, MT5ForConditionalGeneration
from .utils import split_into_paragraphs, process_outputs, filter_outputs

cache_dir = "./model/"
model = MT5ForConditionalGeneration.from_pretrained("snrspeaks/KeyPhraseTransformer", cache_dir=cache_dir)
tokenizer = AutoTokenizer.from_pretrained("snrspeaks/KeyPhraseTransformer", cache_dir=cache_dir)  

def predict(doc: str):
    input_ids = tokenizer.encode(
        doc, return_tensors="pt", add_special_tokens=True
    )
    generated_ids = model.generate(
        input_ids=input_ids,
        num_beams=2,
        max_length=512,
        repetition_penalty=2.5,
        length_penalty=1,
        early_stopping=True,
        top_p=0.95,
        top_k=50,
        num_return_sequences=1,
    )
    preds = [
        tokenizer.decode(
            g, skip_special_tokens=True, clean_up_tokenization_spaces=True
        )
        for g in generated_ids
    ]
    return preds

def extract_keywords(self, text: str, text_block_size: int = 64):
    results = []
    paras = split_into_paragraphs(
        doc=text, tokenizer=tokenizer, max_tokens_per_para=text_block_size
    )

    for para in paras:
        results.append(self.predict(para))

    key_phrases = filter_outputs(process_outputs(results), text)
    return key_phrases

--------------------------------------------------
File End
--------------------------------------------------


./keyword_extraction/__init__.py
File type: .py
from .main import extract_keywords

--------------------------------------------------
File End
--------------------------------------------------


./asr/model.py
File type: .py
import subprocess
import json
from nemo.collections.asr.models import EncDecMultiTaskModel
import moviepy.editor as mp
from pydub import AudioSegment
import os

# load model
canary_model = EncDecMultiTaskModel.from_pretrained('nvidia/canary-1b').to("cuda:0")

# update dcode params
decode_cfg = canary_model.cfg.decoding
decode_cfg.beam.beam_size = 1
canary_model.change_decoding_strategy(decode_cfg)
canary_model.to(canary_model.device)

--------------------------------------------------
File End
--------------------------------------------------


./asr/video_transcription.py
File type: .py
from audio_transcription import transcribe_audio
import moviepy.editor as mp
from pydub import AudioSegment
import os

def convert_video_to_audio(video_path):
    audio_path = video_path.rsplit(".", 1)[0] + ".wav"
    video = mp.VideoFileClip(video_path)

    temp_path = "extracted_audio.wav"
    video.audio.write_audiofile(temp_path)

    sound = AudioSegment.from_wav(temp_path)
    sound = sound.set_channels(1)

    sound.export(audio_path, format="wav")
    os.remove(temp_path)
    return audio_path

def transcribe_video(video_path):
    audio_path = convert_video_to_audio(video_path)
    transcript = transcribe_audio(audio_path)
    return transcript

--------------------------------------------------
File End
--------------------------------------------------


./asr/audio_transcription.py
File type: .py
from .model import canary_model

def transcribe_audio(audio_path):

    transcript = canary_model.transcribe(
        [audio_path],
        batch_size=4,
        # task='asr',
        # source_lang='en',
        # target_lang='en',
        # pnc=True
    )
    return transcript


--------------------------------------------------
File End
--------------------------------------------------


--------------------------------------------------
File End
--------------------------------------------------


./__init__.py
File type: .py


--------------------------------------------------
File End
--------------------------------------------------


./translation.py
File type: .py
from googletrans import Translator
translator = Translator(service_urls=["translate.googleapis.com", "translate.googleapis.co.in"], user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64)')

def translate_text(text, target_language, source_language="auto"):
    translated_text = translator.translate(text, dest=target_language, src=source_language)
    return translated_text.text.strip()

--------------------------------------------------
File End
--------------------------------------------------


./summarization/with_slm.py
File type: .py
from transformers import pipeline
import torch
from features.utils import extract_content_from_file
from pydantic import BaseModel, Field
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, AutoConfig, GenerationConfig
from lmformatenforcer import JsonSchemaParser
from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn
import json

class SummarizationOutput(BaseModel):
    summary: str = Field(description="Summary of the content")
character_level_parser = JsonSchemaParser(SummarizationOutput.model_json_schema())

# Initialize the configs
repo_id = "microsoft/Phi-3.5-mini-instruct"
cache_dir="./model/"
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_4bit_compute_dtype=getattr(torch, "float16"),
    bnb_4bit_quant_type="fp4",
    bnb_4bit_use_double_quant=False
)
config = AutoConfig.from_pretrained(repo_id, cache_dir=cache_dir)

# Init the tokenizer
tokenizer = AutoTokenizer.from_pretrained(repo_id)

# Init generation config and model
prefix_allowed_tokens_fn = build_transformers_prefix_allowed_tokens_fn(tokenizer, character_level_parser)

model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path=repo_id,
    torch_dtype="auto",
    # trust_remote_code=True,
    quantization_config=bnb_config,
    config=config,
    device_map="auto",
)
model.eval()

lm_pipline = pipeline("text-generation", model=model, tokenizer=tokenizer, config=config, framework="pt", device_map="auto", prefix_allowed_tokens_fn=prefix_allowed_tokens_fn)

generate_kwargs = dict(
        max_new_tokens=1024,
        do_sample=True,
        output_scores=True,
        return_dict_in_generate=False,
        return_full_text=False,
)

def summarize_with_slm(file_path):
    text = extract_content_from_file(file_path)
    message = [
        {"role": "system", "content": f"You are a Intelligent summarizer, who give a concise summary of the given content by carefully reading throughout them. You MUST answer using the following json schema: {SummarizationOutput.model_json_schema()}"},
        {"role": "user", "content": f"Content: \n\n{text}"}
    ]

    output = lm_pipline(message, **generate_kwargs)
    json_response = output[0]["generated_text"].strip()
    kv = json.loads(json_response)
    return kv["summary"]

--------------------------------------------------
File End
--------------------------------------------------


./summarization/with_summarization_pipe.py
File type: .py
from transformers import pipeline
from features.utils import extract_content_from_file
from features.utils import get_current_device

model_name = "facebook/bart-large-cnn"
summarizer = pipeline('summarization', model=model_name, framework="pt", device=get_current_device(), cache_dir="./model/")

def summarize_summarizartion_pipeline(file_path):
    text = extract_content_from_file(file_path)[:1024]
    summarized = summarizer(text, max_length=int(0.05 * len(str(text))), min_length=int(0.01 * len(str(text))), do_sample=False)
    summary_text = summarized[0]['summary_text']
    return summary_text

--------------------------------------------------
File End
--------------------------------------------------


./summarization/__init__.py
File type: .py
from with_summarization_pipe import summarize_summarizartion_pipeline
from with_slm import summarize_with_slm

--------------------------------------------------
File End
--------------------------------------------------


./summarization/test.txt
File type: .txt
"The Role of Technology in Modern Society"

In today’s rapidly evolving world, technology plays an essential role in almost every aspect of human life. From communication and transportation to healthcare and education, the advancements in technology have fundamentally reshaped how we live, work, and interact with one another. This transformation can be attributed to several key innovations that have occurred over the past few decades, each of which has contributed to the digital landscape we navigate today.

One of the most significant impacts of technology is in the field of communication. The invention of the internet and the proliferation of smartphones have revolutionized how people communicate, both personally and professionally. Gone are the days when communication was limited to phone calls or letters. With the advent of social media platforms, email, and instant messaging apps, people are now connected globally in ways that were once unimaginable. These technological advancements have not only made communication more efficient but have also democratized access to information, allowing individuals in even the most remote corners of the world to engage with content and participate in global conversations.

In addition to communication, technology has had a profound impact on education. With the rise of online learning platforms, students are no longer confined to traditional classrooms. E-learning platforms, such as Coursera, Khan Academy, and edX, offer a wealth of resources that allow individuals to learn at their own pace, from anywhere in the world. This has been especially valuable during the COVID-19 pandemic, as many schools and universities transitioned to online instruction. The use of artificial intelligence (AI) in education has also enhanced the learning experience, with tools that personalize instruction based on a student's strengths and weaknesses.

Healthcare is another domain that has been transformed by technology. Advances in medical research, diagnostics, and treatment have led to significant improvements in patient outcomes. Wearable technology, such as fitness trackers and smartwatches, allows individuals to monitor their health in real-time, promoting proactive healthcare management. Telemedicine has also become increasingly popular, enabling patients to consult with healthcare professionals remotely, thus reducing the need for in-person visits and improving access to care for individuals in underserved areas.

Moreover, technology has reshaped the business landscape. The rise of e-commerce has revolutionized the way we shop, with online platforms like Amazon and Alibaba providing consumers with unprecedented convenience and access to a global marketplace. The gig economy, powered by technology platforms such as Uber, Airbnb, and Fiverr, has created new opportunities for employment and income generation. Additionally, automation and AI have streamlined operations in industries such as manufacturing, logistics, and finance, leading to increased efficiency and productivity.

However, the rapid advancement of technology also presents challenges. Privacy concerns have come to the forefront as more of our personal information is shared online. Cybersecurity threats, including hacking and identity theft, are on the rise, and governments and organizations are struggling to keep up with the evolving tactics of cybercriminals. Additionally, the digital divide remains a significant issue, with many individuals lacking access to the technology and internet services necessary to participate in the digital economy fully.

Despite these challenges, the future of technology holds immense promise. Emerging technologies such as blockchain, quantum computing, and 5G networks have the potential to further transform society in ways we cannot yet fully comprehend. The ongoing development of AI and machine learning will continue to enhance various industries, from healthcare to finance, making processes more efficient and data-driven.

In conclusion, technology has become an integral part of modern society, driving innovation, shaping industries, and changing how we live. While there are challenges to address, the potential benefits of technological advancements are vast, and their continued evolution will undoubtedly play a crucial role in shaping the future of humanity.

--------------------------------------------------
File End
--------------------------------------------------


./zsclassifier/classifier.py
File type: .py
from transformers import pipeline
from features.utils import get_current_device

pipeline = pipeline("zero-shot-classification", model="facebook/bart-large-mnli", device=get_current_device(), cache_dir="./model") # crosslingual: joeddav/xlm-roberta-large-xnli
sentiment_labels = [
            "anxious",
            "worried",
            "confused",
            "neutral",
            "hopeful",
            "relieved",
            "confident",
            "frustrated",
            "overwhelmed",
            "determined",
            "extreme"
        ]

def classify_text(self, text):
    result = self.pipeline(text, self.sentiment_labels)
    classification_result = result['labels'][0][0]
    return classification_result

--------------------------------------------------
File End
--------------------------------------------------


./zsclassifier/__init__.py
File type: .py
from classifier import classify_text

--------------------------------------------------
File End
--------------------------------------------------


./tts/main.py
File type: .py
import torch
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer, AutoFeatureExtractor
import soundfile as sf
import scipy
from features.utils import get_current_device

repo_id = "parler-tts/parler-tts-mini-v1"
attn_implementation = "sdpa" # "sdpa" 1.4x or "flash_attention_2"
device=get_current_device()

model = ParlerTTSForConditionalGeneration.from_pretrained(
            repo_id,
            attn_implementation=attn_implementation,
            cache_dir="./model/"
        ).to(device)
tokenizer = AutoTokenizer.from_pretrained(repo_id, cache_dir="./model/")
feature_extractor = AutoFeatureExtractor.from_pretrained(repo_id, cache_dir="./model/")

description = "Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise."
input_ids = tokenizer(description, return_tensors="pt").to(model.device)

def text2speech(text: str, file_path):
    prompt_input_ids = tokenizer(text, return_tensors="pt").to(model.device)

    generation = model.generate(
        input_ids=input_ids.input_ids,
        attention_mask=input_ids.attention_mask,
        prompt_input_ids=prompt_input_ids.input_ids,
        prompt_attention_mask=prompt_input_ids.attention_mask,
        do_sample=True,
        return_dict_in_generate=True
    )
    audio_data = generation.sequences[0, :generation.audios_length[0]]

    audio_arr = audio_data.cpu().numpy().squeeze()
    scipy.io.wavfile.write(file_path, rate=feature_extractor.sampling_rate, data=audio_arr)

--------------------------------------------------
File End
--------------------------------------------------


./tts/__init__.py
File type: .py
from main import text2speech

--------------------------------------------------
File End
--------------------------------------------------


./keyword_extraction/utils.py
File type: .py
import nltk
from nltk.corpus import words
from nltk.tokenize import word_tokenize, sent_tokenize

nltk.download('punkt')
nltk.download("words")

def split_into_paragraphs(doc: str, tokenizer,  max_tokens_per_para: int = 128):
    sentences = sent_tokenize(doc.strip())
    temp = ""
    temp_list = []
    final_list = []

    for i, sentence in enumerate(sentences):
        sent = sentence
        temp = temp + " " + sent
        wc_temp = len(tokenizer.tokenize(temp))

        if wc_temp < max_tokens_per_para:
            temp_list.append(sentence)

            if i == len(sentences) - 1:
                final_list.append(" ".join(temp_list))

        else:
            final_list.append(" ".join(temp_list))

            temp = sentence
            temp_list = [sentence]

            if i == len(sentences) - 1:
                final_list.append(" ".join(temp_list))

    return [para for para in final_list if len(para.strip()) != 0]

def process_outputs(outputs):
    temp = [output[0].split(" | ") for output in outputs]
    flatten = [item for sublist in temp for item in sublist]
    return sorted(set(flatten), key=flatten.index)

def filter_outputs(key_phrases, text):
    key_phrases = [elem.lower() for elem in key_phrases]
    text = text.lower()

    valid_phrases = []
    invalid_phrases = []

    for phrases in key_phrases:
        for phrase in word_tokenize(phrases):
            if (phrase in word_tokenize(text)) or (phrase in words.words()):
                if phrases not in valid_phrases:
                    valid_phrases.append(phrases)
            else:
                invalid_phrases.append(phrases)

    return [elem for elem in valid_phrases if elem not in invalid_phrases]

--------------------------------------------------
File End
--------------------------------------------------


./keyword_extraction/main.py
File type: .py
from transformers import AutoTokenizer, T5ForConditionalGeneration, MT5ForConditionalGeneration
from .utils import split_into_paragraphs, process_outputs, filter_outputs

cache_dir = "./model/"
model = MT5ForConditionalGeneration.from_pretrained("snrspeaks/KeyPhraseTransformer", cache_dir=cache_dir)
tokenizer = AutoTokenizer.from_pretrained("snrspeaks/KeyPhraseTransformer", cache_dir=cache_dir)  

def predict(doc: str):
    input_ids = tokenizer.encode(
        doc, return_tensors="pt", add_special_tokens=True
    )
    generated_ids = model.generate(
        input_ids=input_ids,
        num_beams=2,
        max_length=512,
        repetition_penalty=2.5,
        length_penalty=1,
        early_stopping=True,
        top_p=0.95,
        top_k=50,
        num_return_sequences=1,
    )
    preds = [
        tokenizer.decode(
            g, skip_special_tokens=True, clean_up_tokenization_spaces=True
        )
        for g in generated_ids
    ]
    return preds

def extract_keywords(self, text: str, text_block_size: int = 64):
    results = []
    paras = split_into_paragraphs(
        doc=text, tokenizer=tokenizer, max_tokens_per_para=text_block_size
    )

    for para in paras:
        results.append(self.predict(para))

    key_phrases = filter_outputs(process_outputs(results), text)
    return key_phrases

--------------------------------------------------
File End
--------------------------------------------------


./keyword_extraction/__init__.py
File type: .py
from .main import extract_keywords

--------------------------------------------------
File End
--------------------------------------------------


./asr/model.py
File type: .py
import subprocess
import json
from nemo.collections.asr.models import EncDecMultiTaskModel
import moviepy.editor as mp
from pydub import AudioSegment
import os

# load model
canary_model = EncDecMultiTaskModel.from_pretrained('nvidia/canary-1b').to("cuda:0")

# update dcode params
decode_cfg = canary_model.cfg.decoding
decode_cfg.beam.beam_size = 1
canary_model.change_decoding_strategy(decode_cfg)
canary_model.to(canary_model.device)

--------------------------------------------------
File End
--------------------------------------------------


./asr/video_transcription.py
File type: .py
from audio_transcription import transcribe_audio
import moviepy.editor as mp
from pydub import AudioSegment
import os

def convert_video_to_audio(video_path):
    audio_path = video_path.rsplit(".", 1)[0] + ".wav"
    video = mp.VideoFileClip(video_path)

    temp_path = "extracted_audio.wav"
    video.audio.write_audiofile(temp_path)

    sound = AudioSegment.from_wav(temp_path)
    sound = sound.set_channels(1)

    sound.export(audio_path, format="wav")
    os.remove(temp_path)
    return audio_path

def transcribe_video(video_path):
    audio_path = convert_video_to_audio(video_path)
    transcript = transcribe_audio(audio_path)
    return transcript

--------------------------------------------------
File End
--------------------------------------------------


./asr/audio_transcription.py
File type: .py
from .model import canary_model

def transcribe_audio(audio_path):

    transcript = canary_model.transcribe(
        [audio_path],
        batch_size=4,
        # task='asr',
        # source_lang='en',
        # target_lang='en',
        # pnc=True
    )
    return transcript


--------------------------------------------------
File End
--------------------------------------------------
